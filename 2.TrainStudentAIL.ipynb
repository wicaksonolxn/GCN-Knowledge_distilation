{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a865bf-d19f-4803-b256-4a3247bd4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "sw = SummaryWriter(logdir='./Student_AIL', flush_secs=5)\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_scatter import scatter_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d26d62-aed2-4d96-bc93-9efda7479480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size,\n",
    "                            shuffle=True, DEVICE = torch.device('cuda:0')):\n",
    "    '''\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    '''\n",
    "\n",
    "    file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "    filename = os.path.join('./data/PEMS04/', file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) +'_astcgn'\n",
    "    print('load file:', filename)\n",
    "\n",
    "    file_data = np.load(filename + '.npz')\n",
    "    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n",
    "    train_x = train_x[:, :, 0:1, :]\n",
    "    train_target = file_data['train_target']  # (10181, 307, 12)\n",
    "\n",
    "    val_x = file_data['val_x']\n",
    "    val_x = val_x[:, :, 0:1, :]\n",
    "    val_target = file_data['val_target']\n",
    "\n",
    "    test_x = file_data['test_x']\n",
    "    test_x = test_x[:, :, 0:1, :]\n",
    "    test_target = file_data['test_target']\n",
    "\n",
    "    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print\n",
    "    print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_x_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913859a-5cc7-4103-803c-c01c07cc624b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_signal_matrix_filename = './data/PEMS04/pems04.npz'\n",
    "batch_size = 16\n",
    "num_of_weeks = 1\n",
    "num_of_days = 1\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba75207-cb40-4e13-b240-b5db01af551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "        \n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename: # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "\n",
    "        else:\n",
    "         #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8d098-f629-4513-8dd3-a7a8def987b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_filename = None\n",
    "adj_filename = './data/PEMS04/PEMS04.csv'\n",
    "num_of_vertices = 307\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename) #  adj_mx and distance_mx (307, 307)\n",
    "\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "gr = nx.Graph()\n",
    "gr.add_edges_from(edges)\n",
    "# nx.draw(gr, node_size=3)\n",
    "plt.show()\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d96052-6506-427c-85fe-a62e35748109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ASTGCN import ASTGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc7a3f-54c0-4732-9320-a48a0a960f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_block = 2\n",
    "in_channels = 1\n",
    "K = 3\n",
    "nb_chev_filter =64\n",
    "nb_time_filter = 64\n",
    "time_strides = num_of_hours\n",
    "num_for_predict = 12\n",
    "len_input = 12\n",
    "dropout=0.01\n",
    "\n",
    "#L_tilde = scaled_Laplacian(adj_mx)\n",
    "#cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "# net = ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices).to(DEVICE)\n",
    "Teacher=ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices,dropout).to(DEVICE)\n",
    "print(Teacher)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dee06c-5f15-457b-80bf-e9538b7792a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_chev_filterstdn = 64\n",
    "nb_time_filterstdn = 64\n",
    "nb_blockstdn = 1\n",
    "dropouts=0.01\n",
    "Student=ASTGCN( nb_blockstdn, in_channels, K, nb_chev_filterstdn, nb_time_filterstdn, time_strides, num_for_predict, len_input, num_of_vertices,dropouts).to(DEVICE)\n",
    "print(Student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0e88-db3d-4690-baaa-2d5f1939e756",
   "metadata": {},
   "source": [
    "# intialized teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119e88c-aab0-4bbf-84bd-f07093dd3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "learning_rate = 0.001\n",
    "# \n",
    "optimizerTeacher = optim.Adam(Teacher.parameters(), lr=1e-3,weight_decay=1e-6)#l2 regularization applied in weightdecay<0\n",
    "\n",
    "print('Teacher\\'s state_dict:')\n",
    "total_param = 0\n",
    "for param_tensor in Teacher.state_dict():\n",
    "    print(param_tensor, '\\t', Teacher.state_dict()[param_tensor].size(), '\\t', Teacher.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(Teacher.state_dict()[param_tensor].size())\n",
    "print('Teacher\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizerTeacher.state_dict():\n",
    "    print(var_name, '\\t', optimizerTeacher.state_dict()[var_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1180f-7517-4d87-9982-dbd7c64860a9",
   "metadata": {},
   "source": [
    "## Intialized student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae6e1d-3714-4695-bbae-16112488590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "# weight_decay=1e-3\n",
    "optimizerStudent = optim.Adam(Student.parameters(), lr=1e-3,weight_decay=1e-6)\n",
    "\n",
    "print('Teacher\\'s state_dict:')\n",
    "total_param = 0\n",
    "for param_tensor in Student.state_dict():\n",
    "    print(param_tensor, '\\t', Student.state_dict()[param_tensor].size(), '\\t', Student.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(Student.state_dict()[param_tensor].size())\n",
    "print('Student\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizerStudent.state_dict():\n",
    "    print(var_name, '\\t', optimizerStudent.state_dict()[var_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdfc51-7445-4886-9dd8-78649487c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels != null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds - labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ff538-0c64-4191-9752-f5fe61f5d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,ytil,y):\n",
    "        return torch.sqrt(self.mse(ytil,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d4546-b1a4-41fa-9e86-7056d8016980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha=0.1# trade off teacher weights to students, 70% teach 30% stud\n",
    "masked_flag=0\n",
    "criterionStudent=RMSELoss().to(DEVICE)\n",
    "\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mae'\n",
    "\n",
    "metric_method = 'unmask'\n",
    "missing_value=0.0\n",
    "\n",
    "if loss_function=='masked_mse':\n",
    "    criterion_masked = masked_mse         #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag=1\n",
    "elif loss_function=='masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss().to(DEVICE)\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    masked_flag= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fea845-a7da-4200-a5ad-f4e409f4fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ValLoss=[]\n",
    "TrainLoss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc7bee-ed67-447c-b642-27303b61a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, edge_index_data, limit=None):\n",
    "    '''\n",
    "    for rnn, compute mean loss on validation set\n",
    "    :param net: model\n",
    "    :param val_loader: torch.utils.data.utils.DataLoader\n",
    "    :param criterion: torch.nn.MSELoss\n",
    "    :param sw: tensorboardX.SummaryWriter\n",
    "    :param global_step: int, current global_step\n",
    "    :param limit: int,\n",
    "    :return: val_loss\n",
    "    '''\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 20 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        ValLoss.append(validation_loss)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "        print(\"rata rata val_loss\",validation_loss,\"pada epoch:\",epoch)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1642d-f5b7-4524-9ea5-a3856b045e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "best_val_loss = np.inf\n",
    "start_time= time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b437e-8379-420d-a6a8-dea3f043807b",
   "metadata": {},
   "source": [
    "# load weights from teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb20d13-5500-4c21-8a83-371607e2b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =  13\n",
    "params_filename = os.path.join('./Teacher/', 'checkpoint_%s.pth' % epoch)\n",
    "loaded_checkpoint=torch.load(params_filename)\n",
    "epoch=loaded_checkpoint['epoch']\n",
    "Teacher.load_state_dict(loaded_checkpoint['model_state'])\n",
    "optimizerTeacher.load_state_dict(loaded_checkpoint['optimizer_state'])\n",
    "Teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb359b-392a-48fd-8262-0cba113e44dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Teacher.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f6e0e-643f-4cc0-a410-9b5855466d39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var_name in optimizerTeacher.state_dict():\n",
    "    print(var_name, \"\\t\", optimizerTeacher.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c3f73-e09f-4196-803d-fa097b242bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizeN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac541e89-65f4-4bcb-9eb8-185342d506b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=os.path.join('./Teacher/','N.npy')\n",
    "N=np.load(a)\n",
    "N=torch.tensor(N)\n",
    "N.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ab234-fbde-42fd-8c96-fefeee004087",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Student kd\n",
    "Studloss1=[]\n",
    "for epoch in range(40):\n",
    "    params_filename = os.path.join('./Student_AIL/', 'checkpoint_%s.pth' % epoch)\n",
    "    tmp=[]\n",
    "    tmp0=[]\n",
    "    Student.train()  # ensure dropout layers are in train mode\n",
    "    Teacher.eval()\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        encoder_inputs, labels = batch_data   # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "        optimizerStudent.zero_grad()\n",
    "        P_s = Student(encoder_inputs, edge_index_data) # torch.Size([32, 307, 12]) Ps\n",
    "        with torch.no_grad():\n",
    "            P_t=Teacher(encoder_inputs, edge_index_data)# Pt\n",
    "        P_tloss=criterionStudent(P_t,labels)\n",
    "        P_sloss=criterionStudent(P_s,labels)\n",
    "        psi=(1-(P_tloss/N))\n",
    "        loss=P_sloss * alpha + psi*criterionStudent(P_s, P_t) * (1-alpha) #limit terregulasi bergantung dengan output\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizerStudent.step()\n",
    "        distilation_loss = loss.item()\n",
    "        tmp.append(distilation_loss)\n",
    "        tmp0.append(P_sloss.item())\n",
    "        global_step += 1\n",
    "        sw.add_scalar('training_loss', P_sloss, global_step)\n",
    "        if global_step % 100 == 0:\n",
    "            print('global step: %s, Training loss: %.2f, Distilation loss: %.2f, time: %.2fs' % (global_step,P_sloss,distilation_loss, time() - start_time))\n",
    "    studloss=sum(tmp0)/len(tmp0)\n",
    "    rtloss_t=sum(tmp) / len(tmp)\n",
    "    Studloss1.append(studloss)\n",
    "    TrainLoss.append(rtloss_t)\n",
    "    print(\"rata rata Trainingloss: \",studloss,\"rata rata Distilation loss\",rtloss_t,\"pada epoch:\",epoch)\n",
    "    val_loss = compute_val_loss_mstgcn(Student, val_loader, criterionStudent, masked_flag, missing_value, sw, epoch,edge_index_data)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch=epoch\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': best_epoch,\n",
    "            'model_state': Student.state_dict(),\n",
    "            'optimizer_state': optimizerStudent.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, params_filename)\n",
    "        print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "print('best epoch:', best_epoch)\n",
    "path=os.path.join(\"./Student_AIL/\",'losshistval.npy')\n",
    "path1=os.path.join(\"./Student_AIL/\",'losshistdistilation.npy')\n",
    "path2=os.path.join(\"./Student_AIL/\",'losshisttrain.npy')\n",
    "\n",
    "np.save(path,ValLoss)\n",
    "np.save(path1,TrainLoss)\n",
    "np.save(path2,Studloss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865f0c9-f9b5-4d9a-b248-9962dd0e0f42",
   "metadata": {},
   "source": [
    "* label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b823251-7c80-4d2c-a650-cbff40a597de",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =  best\n",
    "params_filename = os.path.join('./Student_AIL/', 'checkpoint_%s.pth' % epoch)\n",
    "loaded_checkpoint=torch.load(params_filename)\n",
    "epoch=loaded_checkpoint['epoch']\n",
    "Student.load_state_dict(loaded_checkpoint['model_state'])\n",
    "optimizerStudent.load_state_dict(loaded_checkpoint['optimizer_state'])\n",
    "Student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5fa17-1c21-453e-9ead-dad7371be2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "            'epoch': best_epoch,\n",
    "            'model_state': Student.state_dict(),\n",
    "            'optimizer_state': optimizerStudent.state_dict()\n",
    "        }\n",
    "torch.save(checkpoint, params_filename)\n",
    "print('save parameters to file: %s' % params_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44449f-1699-4d4b-94c3-b351e783b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time as ps\n",
    "start = ps() \n",
    "Student.train(False)  # ensure dropout layers are in evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = Student(encoder_inputs, edge_index_data)\n",
    "        loss = criterionStudent(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 1 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f ' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "    \n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "end = ps()\n",
    "print(\"rate test : \",end-start, \"second\", \"Test loss : \",test_loss)  \n",
    "    \n",
    "# print(test_loss)\n",
    "np.save('./Student_AIL/test.npy',tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
